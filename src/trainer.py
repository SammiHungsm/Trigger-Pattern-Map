import numpy as np
import evaluate
import torch
from transformers import (
    Trainer, 
    TrainingArguments, 
    DataCollatorForTokenClassification,
    TrainerCallback
)

# 1. è¼‰å…¥ NER å°ˆç”¨è©•ä¼°æŒ‡æ¨™ (seqeval)
# ä½¢æœƒå¹«ä½ è¨ˆ Precision, Recall, F1ï¼Œè€Œå””ä¿‚å–®ç´”å˜… Accuracy
metric = evaluate.load("seqeval")

def compute_metrics(p, id2label):
    """NER å°ˆç”¨æŒ‡æ¨™è¨ˆç®—å‡½æ•¸"""
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # ç§»é™¤ -100 (padding/special tokens) ä¸¦è½‰è¿”åš Label åç¨±
    true_predictions = [
        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

# 2. è‡ªå®šç¾© Debug Callbackï¼šå°‡è¨“ç·´éç¨‹å¯«å…¥ TensorBoard åŒåŸ‹å°å‡ºæ¨£æœ¬
class NERDebugCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics:
            print(f"\nğŸ” [Debug] Step {state.global_step} è©•ä¼°çµæœ: F1={metrics.get('eval_f1', 0):.4f}")

def run_training(model, tokenizer, train_dataset, val_dataset, id2label, output_dir="./results"):
    """
    å„ªåŒ–ç‰ˆ NER è¨“ç·´å™¨
    """
    
    # 1. å®šç¾©è¨“ç·´åƒæ•¸ (åŠ å…¥ TensorBoard åŒåŸ‹ VRAM å„ªåŒ–)
    training_args = TrainingArguments(
        output_dir=output_dir,
        learning_rate=2e-4,
        per_device_train_batch_size=4,   # Large æ¨¡å‹å»ºè­°ç”± 4 é–‹å§‹ï¼Œé˜²æ­¢ OOM
        gradient_accumulation_steps=2,  # ç´¯ç©æ¢¯åº¦ç¶­æŒæœ‰æ•ˆ batch size ç‚º 8
        num_train_epochs=5,
        weight_decay=0.01,
        eval_strategy="steps",          # æ¯éš”ä¸€æ®µæ­¥æ•¸å°±è©•ä¼°ï¼Œå””ä½¿ç­‰å…¨å€‹ epoch
        eval_steps=50,
        save_strategy="steps",
        save_steps=50,
        logging_steps=10,               # æ¯ 10 æ­¥å°± Log ä¸€æ¬¡
        remove_unused_columns=True,     # å¿…è¨­ç‚º True ä»¥é¿å… "str" ValueError
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        fp16=torch.cuda.is_available(), # æœ‰ GPU å°±é–‹ FP16
        # ğŸ”¥ TensorBoard é…ç½®
        report_to="tensorboard",
        logging_dir=f"{output_dir}/logs",
        # ğŸ”¥ VRAM å„ªåŒ–
        gradient_checkpointing=True
    )

    # 2. Data Collator (NER å¿…é ˆç”¨ ForTokenClassification)
    # ä½¢æœƒå¹«ä½ è‡ªå‹•è™•ç† Label å˜… Padding è¨­ç‚º -100
    data_collator = DataCollatorForTokenClassification(
        tokenizer, 
        pad_to_multiple_of=8 if training_args.fp16 else None
    )

    # 3. åˆå§‹åŒ– Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        # å‚³å…¥è‡ªå®šç¾©å˜… compute_metrics (éœ€è¦ id2label)
        compute_metrics=lambda p: compute_metrics(p, id2label),
        callbacks=[NERDebugCallback()]
    )

    # 4. é–‹å§‹è¨“ç·´
    print("ğŸš€ è¨“ç·´å•Ÿå‹•ä¸­... ä½ å¯ä»¥å–ºæ–° Terminal è¼¸å…¥ 'tensorboard --logdir=./results/logs' ç‡åœ–è¡¨")
    trainer.train()
    
    # 5. å„²å­˜
    trainer.save_model(f"{output_dir}/final_model")
    return trainer