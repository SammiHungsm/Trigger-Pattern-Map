# src/trainer.py
import numpy as np
import evaluate
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding

# è¼‰å…¥è©•ä¼°æŒ‡æ¨™ï¼ˆæº–ç¢ºç‡ï¼‰
metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    """è¨ˆç®—æº–ç¢ºç‡å˜… Helper function"""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

def run_training(model, tokenizer, train_dataset, val_dataset, output_dir="./results"):
    """
    å°è£ Trainer é‚è¼¯ï¼Œæ¸›å°‘é‡è¤‡ä»£ç¢¼
    """
    
    # 1. å®šç¾©è¨“ç·´åƒæ•¸
    training_args = TrainingArguments(
        output_dir=output_dir,
        learning_rate=2e-4,            # LoRA å»ºè­°è¼ƒé«˜å˜… LR
        per_device_train_batch_size=8,  # è¦–ä¹ GPU VRAM èª¿æ•´
        per_device_eval_batch_size=8,
        num_train_epochs=5,             # åˆæˆæ•¸æ“šå»ºè­°è¡Œå¤šå¹¾ epoch
        weight_decay=0.01,
        evaluation_strategy="epoch",    # æ¯å€‹ epoch åšä¸€æ¬¡è©•ä¼°
        save_strategy="epoch",          # æ¯å€‹ epoch å„²å­˜ä¸€æ¬¡æ¬Šé‡
        load_best_model_at_end=True,    # è¨“ç·´å®Œè‡ªå‹•è»Šè¿”æœ€å¥½å—°å€‹ version
        logging_steps=10,
        remove_unused_columns=False,    # é‡è¦ï¼šLoRA éœ€è¦ä¿ç•™ä¸€å•² column
        fp16=True,                      # å¦‚æœæœ‰ NVIDIA GPU å°±é–‹ï¼Œå¿«å¥½å¤š
    )

    # 2. Data Collator (è‡ªå‹•å¹«ä½ åš padding)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # 3. åˆå§‹åŒ– Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # 4. é–‹å§‹è¨“ç·´
    print("ğŸš€ è¨“ç·´é–‹å§‹...")
    trainer.train()
    
    # 5. å„²å­˜æœ€çµ‚æ¨¡å‹ (LoRA weights)
    trainer.save_model(f"{output_dir}/final_model")
    print(f"âœ… æ¨¡å‹å·²å„²å­˜è‡³ {output_dir}/final_model")
    
    return trainer